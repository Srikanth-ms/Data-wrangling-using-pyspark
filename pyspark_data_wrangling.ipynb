{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b96c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ca4cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/17 18:37:24 WARN Utils: Your hostname, laptop resolves to a loopback address: 127.0.1.1; using 192.168.208.108 instead (on interface wlo1)\n",
      "24/02/17 18:37:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/17 18:37:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2df45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": 1, \"name\": \"shreyas\", \"age\": 34, \"place\": \"India\"},\n",
    "    {\"id\": 2, \"name\": \"ankit\", \"age\": 30, \"place\": \"USA\"},\n",
    "    {\"id\": 3, \"name\": \"james\", \"age\": 24, \"place\": \"France\"},\n",
    "    {\"id\": 4, \"name\": \"johnson\", \"age\": 38, \"place\": \"Australia\"},\n",
    "    {\"id\": 5, \"name\": \"sachin\", \"age\": 28, \"place\": \"India\"},\n",
    "    {\"id\": 6, \"name\": \"smith\", \"age\": 27, \"place\": \"Australia\"},\n",
    "    {\"id\": 7, \"name\": \"andrew\", \"age\": 33, \"place\": \"USA\"},\n",
    "    {\"id\": 7, \"name\": \"andrew\", \"age\": 33, \"place\": \"USA\"},\n",
    "    {\"id\": 8, \"name\": \"stephen\", \"age\": None, \"place\": None},\n",
    "    {\"id\": 9, \"name\": \"sebastian\", \"age\": None, \"place\": \"Italy\"},\n",
    "]\n",
    "\n",
    "df1 = spark_session.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f22661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+---------+\n",
      "|age |id |name     |place    |\n",
      "+----+---+---------+---------+\n",
      "|34  |1  |shreyas  |India    |\n",
      "|30  |2  |ankit    |USA      |\n",
      "|24  |3  |james    |France   |\n",
      "|38  |4  |johnson  |Australia|\n",
      "|28  |5  |sachin   |India    |\n",
      "|27  |6  |smith    |Australia|\n",
      "|33  |7  |andrew   |USA      |\n",
      "|33  |7  |andrew   |USA      |\n",
      "|NULL|8  |stephen  |NULL     |\n",
      "|NULL|9  |sebastian|Italy    |\n",
      "+----+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(df1.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5648c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35362fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    {\"id\": 1, \"profession\": \"doctor\"},\n",
    "    {\"id\": 2, \"profession\": \"Buisnessman\"},\n",
    "    {\"id\": 3, \"profession\": \"Engineer\"},\n",
    "    {\"id\": 4, \"profession\": \"scientist\"},\n",
    "    {\"id\": 5, \"profession\": \"doctor\"},\n",
    "    {\"id\": 10, \"profession\": \"Manager\"},\n",
    "    {\"id\": 11, \"profession\": \"Accountant\"},\n",
    "]\n",
    "\n",
    "df2 = spark_session.createDataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef09984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |profession |\n",
      "+---+-----------+\n",
      "|1  |doctor     |\n",
      "|2  |Buisnessman|\n",
      "|3  |Engineer   |\n",
      "|4  |scientist  |\n",
      "|5  |doctor     |\n",
      "|10 |Manager    |\n",
      "|11 |Accountant |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(df2.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2602e22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+---------+-----------+\n",
      "|id |age |name     |place    |profession |\n",
      "+---+----+---------+---------+-----------+\n",
      "|1  |34  |shreyas  |India    |doctor     |\n",
      "|2  |30  |ankit    |USA      |Buisnessman|\n",
      "|3  |24  |james    |France   |Engineer   |\n",
      "|5  |28  |sachin   |India    |doctor     |\n",
      "|4  |38  |johnson  |Australia|scientist  |\n",
      "|6  |27  |smith    |Australia|NULL       |\n",
      "|7  |33  |andrew   |USA      |NULL       |\n",
      "|7  |33  |andrew   |USA      |NULL       |\n",
      "|9  |NULL|sebastian|Italy    |NULL       |\n",
      "|8  |NULL|stephen  |NULL     |NULL       |\n",
      "+---+----+---------+---------+-----------+\n",
      "\n",
      "**************************************************\n",
      "RIGHT JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+---------+-----------+\n",
      "|id |age |name   |place    |profession |\n",
      "+---+----+-------+---------+-----------+\n",
      "|1  |34  |shreyas|India    |doctor     |\n",
      "|2  |30  |ankit  |USA      |Buisnessman|\n",
      "|3  |24  |james  |France   |Engineer   |\n",
      "|4  |38  |johnson|Australia|scientist  |\n",
      "|5  |28  |sachin |India    |doctor     |\n",
      "|10 |NULL|NULL   |NULL     |Manager    |\n",
      "|11 |NULL|NULL   |NULL     |Accountant |\n",
      "+---+----+-------+---------+-----------+\n",
      "\n",
      "**************************************************\n",
      "FULL JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+---------+-----------+\n",
      "|id |age |name     |place    |profession |\n",
      "+---+----+---------+---------+-----------+\n",
      "|1  |34  |shreyas  |India    |doctor     |\n",
      "|2  |30  |ankit    |USA      |Buisnessman|\n",
      "|3  |24  |james    |France   |Engineer   |\n",
      "|4  |38  |johnson  |Australia|scientist  |\n",
      "|5  |28  |sachin   |India    |doctor     |\n",
      "|6  |27  |smith    |Australia|NULL       |\n",
      "|7  |33  |andrew   |USA      |NULL       |\n",
      "|7  |33  |andrew   |USA      |NULL       |\n",
      "|8  |NULL|stephen  |NULL     |NULL       |\n",
      "|9  |NULL|sebastian|Italy    |NULL       |\n",
      "|10 |NULL|NULL     |NULL     |Manager    |\n",
      "|11 |NULL|NULL     |NULL     |Accountant |\n",
      "+---+----+---------+---------+-----------+\n",
      "\n",
      "**************************************************\n",
      "INNER JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n",
      "**************************************************\n",
      "LEFT SEMI JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+\n",
      "|id |age|name   |place    |\n",
      "+---+---+-------+---------+\n",
      "|1  |34 |shreyas|India    |\n",
      "|2  |30 |ankit  |USA      |\n",
      "|3  |24 |james  |France   |\n",
      "|4  |38 |johnson|Australia|\n",
      "|5  |28 |sachin |India    |\n",
      "+---+---+-------+---------+\n",
      "\n",
      "**************************************************\n",
      "LEFT ANTI JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+---------+\n",
      "|id |age |name     |place    |\n",
      "+---+----+---------+---------+\n",
      "|6  |27  |smith    |Australia|\n",
      "|7  |33  |andrew   |USA      |\n",
      "|7  |33  |andrew   |USA      |\n",
      "|9  |NULL|sebastian|Italy    |\n",
      "|8  |NULL|stephen  |NULL     |\n",
      "+---+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join() to join two dataframes based \"on\" column and joing type \"how\" \n",
    "df = df1.join(df2, on=\"id\", how=\"left\")\n",
    "df.show(df.count(), truncate=False)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"RIGHT JOIN\")\n",
    "df = df1.join(df2, on=\"id\", how=\"right_outer\")\n",
    "df.show(df.count(), truncate=False)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"FULL JOIN\")\n",
    "df = df1.join(df2, on=\"id\", how=\"full_outer\")\n",
    "df.show(df.count(), truncate=False)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"INNER JOIN\")\n",
    "df = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "df.show(df.count(), truncate=False)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"LEFT SEMI JOIN\")\n",
    "dff = df1.join(df2, on=\"id\", how=\"left_semi\")\n",
    "dff.show(dff.count(), truncate=False)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"LEFT ANTI JOIN\")\n",
    "dff = df1.join(df2, on=\"id\", how=\"left_anti\")\n",
    "dff.show(dff.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdb9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropDuplicates() to drop the duplicate entries based on given subset\n",
    "updated_df = df.dropDuplicates(subset=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3a5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "updated_df.show(df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de6d548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|    place|avg(age)|\n",
      "+---------+--------+\n",
      "|    India|    31.0|\n",
      "|      USA|    32.0|\n",
      "|   France|    24.0|\n",
      "|Australia|    32.5|\n",
      "|     NULL|    NULL|\n",
      "|    Italy|    NULL|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+-----+\n",
      "|    place|count|\n",
      "+---------+-----+\n",
      "|    India|    2|\n",
      "|      USA|    3|\n",
      "|   France|    1|\n",
      "|Australia|    2|\n",
      "|     NULL|    1|\n",
      "|    Italy|    1|\n",
      "+---------+-----+\n",
      "\n",
      "+---------+--------+\n",
      "|    place|min(age)|\n",
      "+---------+--------+\n",
      "|    India|      28|\n",
      "|      USA|      30|\n",
      "|   France|      24|\n",
      "|Australia|      27|\n",
      "|     NULL|    NULL|\n",
      "|    Italy|    NULL|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+-------+-------+-------+---------+------------------+-------+----------------+\n",
      "|    place|avg_age|min_age|max_age|count_age|distinct_count_age|sum_age|distinct_sum_age|\n",
      "+---------+-------+-------+-------+---------+------------------+-------+----------------+\n",
      "|   France|   24.0|     24|     24|        1|                 1|     24|              24|\n",
      "|     NULL|   NULL|   NULL|   NULL|        0|                 0|   NULL|            NULL|\n",
      "|    India|   31.0|     28|     34|        2|                 2|     62|              62|\n",
      "|    Italy|   NULL|   NULL|   NULL|        0|                 0|   NULL|            NULL|\n",
      "|      USA|   32.0|     30|     33|        3|                 2|     96|              63|\n",
      "|Australia|   32.5|     27|     38|        2|                 2|     65|              65|\n",
      "+---------+-------+-------+-------+---------+------------------+-------+----------------+\n",
      "\n",
      "+--------+----------+-------------------+--------+--------+--------+-----------------+\n",
      "|avg(age)|count(age)|count(DISTINCT age)|min(age)|max(age)|sum(age)|sum(DISTINCT age)|\n",
      "+--------+----------+-------------------+--------+--------+--------+-----------------+\n",
      "|  30.875|         8|                  7|      24|      38|     247|              214|\n",
      "+--------+----------+-------------------+--------+--------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, max, sum, sum_distinct, count, count_distinct\n",
    "\n",
    "#use groupBy() for grouping and performing aggregation on columns based on grouped data\n",
    "df1.groupBy([\"place\"]).agg({'age': 'avg'}).show()\n",
    "\n",
    "# count()\n",
    "df1.groupBy([\"place\"]).count().show()\n",
    "\n",
    "# avg()\n",
    "df1.groupBy([\"place\"]).min(\"age\").show()\n",
    "\n",
    "# multiple aggegation functions for agg()\n",
    "df1.groupBy([\"place\"]).agg(avg(\"age\").alias(\"avg_age\"),\n",
    "                                  min(\"age\").alias(\"min_age\"),\n",
    "                                  max(\"age\").alias(\"max_age\"),\n",
    "                                  count(\"age\").alias(\"count_age\"),\n",
    "                                  count_distinct(\"age\").alias(\"distinct_count_age\"),\n",
    "                                  sum(\"age\").alias(\"sum_age\"),\n",
    "                                  sum_distinct(\"age\").alias(\"distinct_sum_age\")).show()\n",
    "\n",
    "# perform aggregation on columns without grouping\n",
    "df1.select(avg(\"age\"), count(\"age\"), count_distinct(\"age\"), min(\"age\"), max(\"age\"),\n",
    "                  sum(\"age\"), sum_distinct(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "addfbdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+---------+\n",
      "| age| id|     name|    place|\n",
      "+----+---+---------+---------+\n",
      "|  34|  1|  shreyas|    India|\n",
      "|  30|  2|    ankit|      USA|\n",
      "|  24|  3|    james|   France|\n",
      "|  38|  4|  johnson|Australia|\n",
      "|  28|  5|   sachin|    India|\n",
      "|  27|  6|    smith|Australia|\n",
      "|  33|  7|   andrew|      USA|\n",
      "|  33|  7|   andrew|      USA|\n",
      "|NULL|  8|  stephen|     NULL|\n",
      "|NULL|  9|sebastian|    Italy|\n",
      "+----+---+---------+---------+\n",
      "\n",
      "+---+---+-------+---------+\n",
      "|age| id|   name|    place|\n",
      "+---+---+-------+---------+\n",
      "| 34|  1|shreyas|    India|\n",
      "| 30|  2|  ankit|      USA|\n",
      "| 24|  3|  james|   France|\n",
      "| 38|  4|johnson|Australia|\n",
      "| 28|  5| sachin|    India|\n",
      "| 27|  6|  smith|Australia|\n",
      "| 33|  7| andrew|      USA|\n",
      "| 33|  7| andrew|      USA|\n",
      "+---+---+-------+---------+\n",
      "\n",
      "+----+---+---------+---------+\n",
      "| age| id|     name|    place|\n",
      "+----+---+---------+---------+\n",
      "|  34|  1|  shreyas|    India|\n",
      "|  30|  2|    ankit|      USA|\n",
      "|  24|  3|    james|   France|\n",
      "|  38|  4|  johnson|Australia|\n",
      "|  28|  5|   sachin|    India|\n",
      "|  27|  6|    smith|Australia|\n",
      "|  33|  7|   andrew|      USA|\n",
      "|  33|  7|   andrew|      USA|\n",
      "|NULL|  9|sebastian|    Italy|\n",
      "+----+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using dropna to drop null values\n",
    "new_df = spark_session.createDataFrame(data, schema=[\"age\", \"id\", \"name\", \"place\"])\n",
    "new_df.show()\n",
    "\n",
    "# If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "new_df.dropna(subset=[\"place\", \"age\"], how=\"any\").show()\n",
    "new_df.dropna(subset=[\"place\", \"age\"], how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63a39c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+----------+\n",
      "|id |age|name |place |profession|\n",
      "+---+---+-----+------+----------+\n",
      "|3  |24 |james|France|Engineer  |\n",
      "+---+---+-----+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+-----+----------+\n",
      "|id |age|name   |place|profession|\n",
      "+---+---+-------+-----+----------+\n",
      "|1  |34 |shreyas|India|doctor    |\n",
      "|5  |28 |sachin |India|doctor    |\n",
      "+---+---+-------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+----------+\n",
      "|id |age|name   |place    |profession|\n",
      "+---+---+-------+---------+----------+\n",
      "|4  |38 |johnson|Australia|scientist |\n",
      "+---+---+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+----------+\n",
      "|id |age|name |place |profession|\n",
      "+---+---+-----+------+----------+\n",
      "|3  |24 |james|France|Engineer  |\n",
      "+---+---+-----+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+-----+----------+\n",
      "|id |age|name   |place|profession|\n",
      "+---+---+-------+-----+----------+\n",
      "|1  |34 |shreyas|India|doctor    |\n",
      "|5  |28 |sachin |India|doctor    |\n",
      "+---+---+-------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+----------+\n",
      "|id |age|name   |place    |profession|\n",
      "+---+---+-------+---------+----------+\n",
      "|3  |24 |james  |France   |Engineer  |\n",
      "|4  |38 |johnson|Australia|scientist |\n",
      "+---+---+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# here we can use where() instead of filter, both are same\n",
    "#using filter() for filtering of data \n",
    "updated_df.filter(~(updated_df[\"profession\"]=='doctor') & ~(updated_df[\"age\"]> 25)).show(updated_df.count(), truncate=False)\n",
    "\n",
    "# filter using startswith()\n",
    "updated_df.filter(updated_df[\"profession\"].startswith('d')).show(updated_df.count(), truncate=False)\n",
    "\n",
    "# filter using endswith()\n",
    "updated_df.filter(updated_df[\"profession\"].endswith('st')).show(updated_df.count(), truncate=False)\n",
    "\n",
    "# filter using like()\n",
    "updated_df.filter(updated_df[\"name\"].like('ja%')).show(updated_df.count(), truncate=False)\n",
    "\n",
    "# filter using contains()\n",
    "updated_df.filter(updated_df[\"profession\"].contains(\"doc\")).show(updated_df.count(), truncate=False)\n",
    "\n",
    "# filter using isin()\n",
    "li = [\"scientist\", \"Engineer\"]\n",
    "updated_df.filter(updated_df[\"profession\"].isin(li)).show(updated_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "966c9355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+-----------+\n",
      "|name   |age|place    |profession |\n",
      "+-------+---+---------+-----------+\n",
      "|shreyas|34 |India    |doctor     |\n",
      "|ankit  |30 |USA      |Buisnessman|\n",
      "|james  |24 |France   |Engineer   |\n",
      "|johnson|38 |Australia|scientist  |\n",
      "|sachin |28 |India    |doctor     |\n",
      "+-------+---+---------+-----------+\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+-----------+\n",
      "|name   |age|place    |profession |\n",
      "+-------+---+---------+-----------+\n",
      "|shreyas|34 |India    |doctor     |\n",
      "|ankit  |30 |USA      |Buisnessman|\n",
      "|james  |24 |France   |Engineer   |\n",
      "|johnson|38 |Australia|scientist  |\n",
      "|sachin |28 |India    |doctor     |\n",
      "+-------+---+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 251:==========================================>              (6 + 2) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select column names for display\n",
    "updated_df.select(updated_df[\"name\"], updated_df[\"age\"], updated_df[\"place\"], updated_df[\"profession\"]).show(updated_df.count(), False)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# OR\n",
    "updated_df.select([\"name\", \"age\", \"place\", \"profession\"]).show(updated_df.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3890d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[453] at readRDDFromFile at PythonRDD.scala:289\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"shreyas\", 34, \"India\"),\n",
    "    (\"ankit\", 30, \"USA\"),\n",
    "    (\"james\", 24, \"France\"),\n",
    "    (\"johnson\", 38, \"Australia\"),\n",
    "    (\"sachin\", 28, \"India\"),\n",
    "    (\"smith\", 27, \"Australia\"),\n",
    "    (\"andrew\", 33, \"USA\"),\n",
    "    (\"stephen\", None, None),\n",
    "    (\"sebastian\", None, \"Italy\"),\n",
    "]\n",
    "\n",
    "schema = [\"name\", \"age\", \"place\"]\n",
    "\n",
    "# Create RDD using sparkContext.parallelize()\n",
    "rdd_data = spark_session.sparkContext.parallelize(data)\n",
    "print(rdd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6dafa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using toDF() to convert RDD to dataframe....\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      "\n",
      "+---------+----+---------+\n",
      "|name     |age |place    |\n",
      "+---------+----+---------+\n",
      "|shreyas  |34  |India    |\n",
      "|ankit    |30  |USA      |\n",
      "|james    |24  |France   |\n",
      "|johnson  |38  |Australia|\n",
      "|sachin   |28  |India    |\n",
      "|smith    |27  |Australia|\n",
      "|andrew   |33  |USA      |\n",
      "|stephen  |NULL|NULL     |\n",
      "|sebastian|NULL|Italy    |\n",
      "+---------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert RDD to Dataframe using toDF()\n",
    "print(\"Using toDF() to convert RDD to dataframe....\")\n",
    "rdd_converted_toDF = rdd_data.toDF(schema=schema)\n",
    "\n",
    "rdd_converted_toDF.printSchema()\n",
    "rdd_converted_toDF.show(rdd_converted_toDF.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165b64e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using createDataFrame() to convert RDD to dataframe....\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      "\n",
      "+---------+----+---------+\n",
      "|name     |age |place    |\n",
      "+---------+----+---------+\n",
      "|shreyas  |34  |India    |\n",
      "|ankit    |30  |USA      |\n",
      "|james    |24  |France   |\n",
      "|johnson  |38  |Australia|\n",
      "|sachin   |28  |India    |\n",
      "|smith    |27  |Australia|\n",
      "|andrew   |33  |USA      |\n",
      "|stephen  |NULL|NULL     |\n",
      "|sebastian|NULL|Italy    |\n",
      "+---------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert RDD to Dataframe using createDataFrame()\n",
    "print(\"Using createDataFrame() to convert RDD to dataframe....\")\n",
    "rdd_converted_createDataFrame = spark_session.createDataFrame(rdd_data, schema=schema)\n",
    "\n",
    "rdd_converted_createDataFrame.printSchema()\n",
    "rdd_converted_createDataFrame.show(rdd_converted_createDataFrame.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dbbf592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name   age      place\n",
      "0    shreyas  34.0      India\n",
      "1      ankit  30.0        USA\n",
      "2      james  24.0     France\n",
      "3    johnson  38.0  Australia\n",
      "4     sachin  28.0      India\n",
      "5      smith  27.0  Australia\n",
      "6     andrew  33.0        USA\n",
      "7    stephen   NaN       None\n",
      "8  sebastian   NaN      Italy\n"
     ]
    }
   ],
   "source": [
    "# convert to pyspark dataframe into pandas dataframe\n",
    "pandas_df = rdd_converted_createDataFrame.toPandas()\n",
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba3a8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before using distinct()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n",
      "Data after using distinct()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 294:=================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# distinct() is similar to dropDuplicates() but we can't select columns while dropping duplicates\n",
    "print(\"Data before using distinct()...\")\n",
    "df.show(df.count(), truncate=False)\n",
    "\n",
    "# distinct() to get distinct rows\n",
    "print(\"Data after using distinct()...\")\n",
    "distinct_df = df.distinct()\n",
    "distinct_df.show(distinct_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b619f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 322:===================================>                     (5 + 3) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#sort() and orderBy() to sort data\n",
    "sorted_df = updated_df.orderBy([\"name\", \"place\"], ascending=[True, False])\n",
    "sorted_df.show(sorted_df.count(), truncate=False)\n",
    "\n",
    "#sort\n",
    "sorted_df = updated_df.sort([\"name\", \"place\"], ascending=[True, False])\n",
    "sorted_df.show(sorted_df.count(), truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "babbfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 336:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fillna() to fill null values in dataframe\n",
    "updated_df = df.fillna(subset=[\"name\", \"place\"], value=\"unknown\")\n",
    "updated_df.show(updated_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a1c03fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+\n",
      "|id |place    |profession |\n",
      "+---+---------+-----------+\n",
      "|1  |India    |doctor     |\n",
      "|2  |USA      |Buisnessman|\n",
      "|3  |France   |Engineer   |\n",
      "|4  |Australia|scientist  |\n",
      "|5  |India    |doctor     |\n",
      "+---+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop() to drop columns in dataframe\n",
    "updated_df1 = df.drop(\"name\", \"age\")\n",
    "updated_df1.show(updated_df1.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60c310ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 355:=================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-----------+\n",
      "|id |age|name   |place    |profession |\n",
      "+---+---+-------+---------+-----------+\n",
      "|1  |34 |shreyas|India    |doctor     |\n",
      "|2  |30 |ankit  |USA      |Buisnessman|\n",
      "|3  |24 |james  |France   |Engineer   |\n",
      "|4  |38 |johnson|Australia|scientist  |\n",
      "|5  |28 |sachin |India    |doctor     |\n",
      "+---+---+-------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 364:==============>                                          (2 + 6) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# replace() to replace value with specific value in dataframe\n",
    "updated_df = df.replace(subset=[\"name\"], to_replace=\"andrew\", value=\"Andrew\")\n",
    "updated_df.show(updated_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "071822bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, age=34, name='shreyas', place='India', profession='doctor'),\n",
       " Row(id=2, age=30, name='ankit', place='USA', profession='Buisnessman'),\n",
       " Row(id=3, age=24, name='james', place='France', profession='Engineer')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use head() to display n rows from first\n",
    "df1 = df\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0b68d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=3, age=24, name='james', place='France', profession='Engineer'),\n",
       " Row(id=4, age=38, name='johnson', place='Australia', profession='scientist'),\n",
       " Row(id=5, age=28, name='sachin', place='India', profession='doctor')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use tail() to display n rows from last\n",
    "df2 = df\n",
    "df2.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f8f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'age', 'name', 'place', 'profession']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# to get columns and number of columns\n",
    "print(df.columns)\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc447ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 379:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|                5|\n",
      "|   mean|             30.8|\n",
      "| stddev|5.403702434442519|\n",
      "|    min|               24|\n",
      "|    max|               38|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# describe() to show statistics results\n",
    "df.describe('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24ea8f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of distinct places\n",
    "df.select(['place']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be0d963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 420:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------+------+---------+\n",
      "|place_profession|Buisnessman|Engineer|doctor|scientist|\n",
      "+----------------+-----------+--------+------+---------+\n",
      "|          France|          0|       1|     0|        0|\n",
      "|           India|          0|       0|     2|        0|\n",
      "|             USA|          1|       0|     0|        0|\n",
      "|       Australia|          0|       0|     0|        1|\n",
      "+----------------+-----------+--------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate Pair-wise Frequency of Categorical Columns using crosstab()\n",
    "df.crosstab(\"place\", \"profession\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "569babdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+-----------------+-------+\n",
      "| id|    name|salary|age|      designation|dept_id|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "|  1|   Edith|100000| 35|Software Engineer|      2|\n",
      "|  2|  tyrion|120000| 25|       Programmer|      4|\n",
      "|  3|     jon| 51000| 37|  System Engineer|      4|\n",
      "|  4|daenerys| 70000| 31|        Developer|      1|\n",
      "|  5|    Andy| 20000| 41| Senior Developer|      2|\n",
      "|  6|    Luis| 40000| 25|        Developer|      3|\n",
      "|  7|  Keshav| 50000| 36|       Programmer|      2|\n",
      "|  8|   david| 60000| 42|  System Engineer|      1|\n",
      "|  9|  Madhav| 40000| 22|Software Engineer|      3|\n",
      "| 10|   Jadav| 70000| 32|Software Engineer|   NULL|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read csv file as df with chaining of options\n",
    "\n",
    "csv_data1 = spark_session.read.option(\"header\", True) \\\n",
    "                             .option(\"inferSchema\", True) \\\n",
    "                             .option(\"delimiter\", \",\") \\\n",
    "                             .csv(\"/home/laptop/Downloads/employee.csv\")\n",
    "\n",
    "csv_data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f644ed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+-----------------+-------+\n",
      "| id|    name|salary|age|      designation|dept_id|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "|  1|   Edith|100000| 35|Software Engineer|      2|\n",
      "|  2|  tyrion|120000| 25|       Programmer|      4|\n",
      "|  3|     jon| 51000| 37|  System Engineer|      4|\n",
      "|  4|daenerys| 70000| 31|        Developer|      1|\n",
      "|  5|    Andy| 20000| 41| Senior Developer|      2|\n",
      "|  6|    Luis| 40000| 25|        Developer|      3|\n",
      "|  7|  Keshav| 50000| 36|       Programmer|      2|\n",
      "|  8|   david| 60000| 42|  System Engineer|      1|\n",
      "|  9|  Madhav| 40000| 22|Software Engineer|      3|\n",
      "| 10|   Jadav| 70000| 32|Software Engineer|   NULL|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read csv file as df without chaining of options\n",
    "\n",
    "csv_data2 = spark_session.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                             .csv(\"/home/laptop/Downloads/employee.csv\")\n",
    "\n",
    "csv_data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47bc2b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+------------+------+--------------+\n",
      "|Date      |Decimal Date|Average|Interpolated|Trend |Number of Days|\n",
      "+----------+------------+-------+------------+------+--------------+\n",
      "|1958-03-01|1958.208    |315.71 |315.71      |314.62|-1            |\n",
      "|1958-04-01|1958.292    |317.45 |317.45      |315.29|-1            |\n",
      "|1958-05-01|1958.375    |317.50 |317.50      |314.71|-1            |\n",
      "|1958-06-01|1958.458    |-99.99 |317.10      |314.85|-1            |\n",
      "|1958-07-01|1958.542    |315.86 |315.86      |314.98|-1            |\n",
      "|1958-08-01|1958.625    |314.93 |314.93      |315.94|-1            |\n",
      "|1958-09-01|1958.708    |313.20 |313.20      |315.91|-1            |\n",
      "|1958-10-01|1958.792    |-99.99 |312.66      |315.61|-1            |\n",
      "|1958-11-01|1958.875    |313.33 |313.33      |315.31|-1            |\n",
      "|1958-12-01|1958.958    |314.67 |314.67      |315.61|-1            |\n",
      "+----------+------------+-------+------------+------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read folder containing csv files as df\n",
    "csv_data3 = spark_session.read.option(\"header\", True).csv(\"/home/laptop/Downloads/csv_files/\")\n",
    "csv_data3.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61214fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df to csv\n",
    "csv_data2.write.options(inferSchema='True', delimiter=',', header='True') \\\n",
    "                   .mode(\"ignore\").csv('/home/laptop/Downloads/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c77cd6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write df to parquet file without partition\n",
    "csv_data1.write.mode(\"overwrite\").parquet(\"/home/laptop/Downloads/test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a58e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df to paraquet file using partitionBy()\n",
    "csv_data1.write.partitionBy(\"age\").mode(\"overwrite\").parquet(\"/home/laptop/Downloads/test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eae43260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----------+-------+\n",
      "| id|  name|salary|designation|dept_id|\n",
      "+---+------+------+-----------+-------+\n",
      "|  2|tyrion|120000| Programmer|      4|\n",
      "|  6|  Luis| 40000|  Developer|      3|\n",
      "+---+------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read parquet file containing partition\n",
    "parquet_data1 = spark_session.read.parquet(\"/home/laptop/Downloads/test2/age=25/\")\n",
    "parquet_data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d49d07b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+-----------------+-------+\n",
      "| id|    name|salary|age|      designation|dept_id|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "|  1|   Edith|100000| 35|Software Engineer|      2|\n",
      "|  2|  tyrion|120000| 25|       Programmer|      4|\n",
      "|  3|     jon| 51000| 37|  System Engineer|      4|\n",
      "|  4|daenerys| 70000| 31|        Developer|      1|\n",
      "|  5|    Andy| 20000| 41| Senior Developer|      2|\n",
      "|  6|    Luis| 40000| 25|        Developer|      3|\n",
      "|  7|  Keshav| 50000| 36|       Programmer|      2|\n",
      "|  8|   david| 60000| 42|  System Engineer|      1|\n",
      "|  9|  Madhav| 40000| 22|Software Engineer|      3|\n",
      "| 10|   Jadav| 70000| 32|Software Engineer|   NULL|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "\n",
      "+---+-------------+---------------+------------+-----------------+-------+\n",
      "| id|employee_name|employee_salary|employee_age|      designation|dept_id|\n",
      "+---+-------------+---------------+------------+-----------------+-------+\n",
      "|  1|        Edith|         100000|          35|Software Engineer|      2|\n",
      "|  2|       tyrion|         120000|          25|       Programmer|      4|\n",
      "|  3|          jon|          51000|          37|  System Engineer|      4|\n",
      "|  4|     daenerys|          70000|          31|        Developer|      1|\n",
      "|  5|         Andy|          20000|          41| Senior Developer|      2|\n",
      "|  6|         Luis|          40000|          25|        Developer|      3|\n",
      "|  7|       Keshav|          50000|          36|       Programmer|      2|\n",
      "|  8|        david|          60000|          42|  System Engineer|      1|\n",
      "|  9|       Madhav|          40000|          22|Software Engineer|      3|\n",
      "| 10|        Jadav|          70000|          32|Software Engineer|   NULL|\n",
      "+---+-------------+---------------+------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_data2.show()\n",
    "\n",
    "# withColumnRenamed()\n",
    "column_renamed = csv_data2.withColumnRenamed(\"name\", \"employee_name\") \\\n",
    "                          .withColumnRenamed(\"age\", \"employee_age\") \\\n",
    "                          .withColumnRenamed(\"salary\", \"employee_salary\")\n",
    "\n",
    "column_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7347d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+-----------------+-------+\n",
      "| id|    name|salary|age|      designation|dept_id|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "|  1|   Edith|100000| 35|Software Engineer|      2|\n",
      "|  2|  tyrion|120000| 25|       Programmer|      4|\n",
      "|  3|     jon| 51000| 37|  System Engineer|      4|\n",
      "|  4|daenerys| 70000| 31|        Developer|      1|\n",
      "|  5|    Andy| 20000| 41| Senior Developer|      2|\n",
      "|  6|    Luis| 40000| 25|        Developer|      3|\n",
      "|  7|  Keshav| 50000| 36|       Programmer|      2|\n",
      "|  8|   david| 60000| 42|  System Engineer|      1|\n",
      "|  9|  Madhav| 40000| 22|Software Engineer|      3|\n",
      "| 10|   Jadav| 70000| 32|Software Engineer|   NULL|\n",
      "+---+--------+------+---+-----------------+-------+\n",
      "\n",
      "+------+--------+----------+-------+-----------------+-------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|\n",
      "+------+--------+----------+-------+-----------------+-------+\n",
      "|     1|   Edith|    100000|     35|Software Engineer|      2|\n",
      "|     2|  tyrion|    120000|     25|       Programmer|      4|\n",
      "|     3|     jon|     51000|     37|  System Engineer|      4|\n",
      "|     4|daenerys|     70000|     31|        Developer|      1|\n",
      "|     5|    Andy|     20000|     41| Senior Developer|      2|\n",
      "|     6|    Luis|     40000|     25|        Developer|      3|\n",
      "|     7|  Keshav|     50000|     36|       Programmer|      2|\n",
      "|     8|   david|     60000|     42|  System Engineer|      1|\n",
      "|     9|  Madhav|     40000|     22|Software Engineer|      3|\n",
      "|    10|   Jadav|     70000|     32|Software Engineer|   NULL|\n",
      "+------+--------+----------+-------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_data2.show()\n",
    "\n",
    "# to rename all columns using toDF()\n",
    "columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_age\", \"emp_designation\", \"dept_id\"]\n",
    "all_columns_renamed = csv_data2.toDF(*columns)\n",
    "all_columns_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84383edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+-----------------+-------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|\n",
      "+------+--------+----------+-------+-----------------+-------+\n",
      "|     1|   Edith|    100000|     35|Software Engineer|      2|\n",
      "|     2|  tyrion|    120000|     25|       Programmer|      4|\n",
      "|     3|     jon|     51000|     37|  System Engineer|      4|\n",
      "|     4|daenerys|     70000|     31|        Developer|      1|\n",
      "|     5|    Andy|     20000|     41| Senior Developer|      2|\n",
      "|     6|    Luis|     40000|     25|        Developer|      3|\n",
      "|     7|  Keshav|     50000|     36|       Programmer|      2|\n",
      "|     8|   david|     60000|     42|  System Engineer|      1|\n",
      "|     9|  Madhav|     40000|     22|Software Engineer|      3|\n",
      "|    10|   Jadav|     70000|     32|Software Engineer|   NULL|\n",
      "+------+--------+----------+-------+-----------------+-------+\n",
      "\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|     1|   Edith|    100000|     35|Software Engineer|      2|    150000.0|\n",
      "|     2|  tyrion|    120000|     25|       Programmer|      4|    180000.0|\n",
      "|     3|     jon|     51000|     37|  System Engineer|      4|     76500.0|\n",
      "|     4|daenerys|     70000|     31|        Developer|      1|    105000.0|\n",
      "|     5|    Andy|     20000|     41| Senior Developer|      2|     30000.0|\n",
      "|     6|    Luis|     40000|     25|        Developer|      3|     60000.0|\n",
      "|     7|  Keshav|     50000|     36|       Programmer|      2|     75000.0|\n",
      "|     8|   david|     60000|     42|  System Engineer|      1|     90000.0|\n",
      "|     9|  Madhav|     40000|     22|Software Engineer|      3|     60000.0|\n",
      "|    10|   Jadav|     70000|     32|Software Engineer|   NULL|    105000.0|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_columns_renamed.show()\n",
    "\n",
    "# withColumn() to add new column by utilizing existing column\n",
    "hiked_salary = all_columns_renamed.withColumn(\"hiked_salary\", all_columns_renamed['emp_salary']* 1.50)\n",
    "hiked_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ac3689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|     1|   Edith|    100000|     35|Software Engineer|      2|    150000.0|\n",
      "|     2|  tyrion|    120000|     25|       Programmer|      4|    180000.0|\n",
      "|     3|     jon|     51000|     37|  System Engineer|      4|     76500.0|\n",
      "|     4|daenerys|     70000|     31|        Developer|      1|    105000.0|\n",
      "|     5|    Andy|     20000|     41| Senior Developer|      2|     30000.0|\n",
      "|     6|    Luis|     40000|     25|        Developer|      3|     60000.0|\n",
      "|     7|  Keshav|     50000|     36|       Programmer|      2|     75000.0|\n",
      "|     8|   david|     60000|     42|  System Engineer|      1|     90000.0|\n",
      "|     9|  Madhav|     40000|     22|Software Engineer|      3|     60000.0|\n",
      "|    10|   Jadav|     70000|     32|Software Engineer|   NULL|    105000.0|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|     1|   Edith|  100000.0|     35|Software Engineer|      2|    150000.0|\n",
      "|     2|  tyrion|  120000.0|     25|       Programmer|      4|    180000.0|\n",
      "|     3|     jon|   51000.0|     37|  System Engineer|      4|     76500.0|\n",
      "|     4|daenerys|   70000.0|     31|        Developer|      1|    105000.0|\n",
      "|     5|    Andy|   20000.0|     41| Senior Developer|      2|     30000.0|\n",
      "|     6|    Luis|   40000.0|     25|        Developer|      3|     60000.0|\n",
      "|     7|  Keshav|   50000.0|     36|       Programmer|      2|     75000.0|\n",
      "|     8|   david|   60000.0|     42|  System Engineer|      1|     90000.0|\n",
      "|     9|  Madhav|   40000.0|     22|Software Engineer|      3|     60000.0|\n",
      "|    10|   Jadav|   70000.0|     32|Software Engineer|   NULL|    105000.0|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiked_salary.show()\n",
    "\n",
    "# withColumn() to type cast existing column\n",
    "type_casted_column = hiked_salary.withColumn(\"emp_salary\", hiked_salary[\"emp_salary\"].cast(\"Float\"))\n",
    "type_casted_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb29b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|     1|   Edith|  100000.0|     35|Software Engineer|      2|    150000.0|\n",
      "|     2|  tyrion|  120000.0|     25|       Programmer|      4|    180000.0|\n",
      "|     3|     jon|   51000.0|     37|  System Engineer|      4|     76500.0|\n",
      "|     4|daenerys|   70000.0|     31|        Developer|      1|    105000.0|\n",
      "|     5|    Andy|   20000.0|     41| Senior Developer|      2|     30000.0|\n",
      "|     6|    Luis|   40000.0|     25|        Developer|      3|     60000.0|\n",
      "|     7|  Keshav|   50000.0|     36|       Programmer|      2|     75000.0|\n",
      "|     8|   david|   60000.0|     42|  System Engineer|      1|     90000.0|\n",
      "|     9|  Madhav|   40000.0|     22|Software Engineer|      3|     60000.0|\n",
      "|    10|   Jadav|   70000.0|     32|Software Engineer|   NULL|    105000.0|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|     1|   Edith|  100000.0|     35|Software Engineer|      2|    112500.0|\n",
      "|     2|  tyrion|  120000.0|     25|       Programmer|      4|    135000.0|\n",
      "|     3|     jon|   51000.0|     37|  System Engineer|      4|     57375.0|\n",
      "|     4|daenerys|   70000.0|     31|        Developer|      1|     78750.0|\n",
      "|     5|    Andy|   20000.0|     41| Senior Developer|      2|     22500.0|\n",
      "|     6|    Luis|   40000.0|     25|        Developer|      3|     45000.0|\n",
      "|     7|  Keshav|   50000.0|     36|       Programmer|      2|     56250.0|\n",
      "|     8|   david|   60000.0|     42|  System Engineer|      1|     67500.0|\n",
      "|     9|  Madhav|   40000.0|     22|Software Engineer|      3|     45000.0|\n",
      "|    10|   Jadav|   70000.0|     32|Software Engineer|   NULL|     78750.0|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type_casted_column.show()\n",
    "\n",
    "# withColumn() to update data of existing column \n",
    "updated_salary = type_casted_column.withColumn(\"hiked_salary\", type_casted_column[\"hiked_salary\"]*0.75)\n",
    "updated_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "550b9445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "|     1|   Edith|  100000.0|     35|Software Engineer|      2|    112500.0|\n",
      "|     2|  tyrion|  120000.0|     25|       Programmer|      4|    135000.0|\n",
      "|     3|     jon|   51000.0|     37|  System Engineer|      4|     57375.0|\n",
      "|     4|daenerys|   70000.0|     31|        Developer|      1|     78750.0|\n",
      "|     5|    Andy|   20000.0|     41| Senior Developer|      2|     22500.0|\n",
      "|     6|    Luis|   40000.0|     25|        Developer|      3|     45000.0|\n",
      "|     7|  Keshav|   50000.0|     36|       Programmer|      2|     56250.0|\n",
      "|     8|   david|   60000.0|     42|  System Engineer|      1|     67500.0|\n",
      "|     9|  Madhav|   40000.0|     22|Software Engineer|      3|     45000.0|\n",
      "|    10|   Jadav|   70000.0|     32|Software Engineer|   NULL|     78750.0|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+\n",
      "\n",
      "+------+--------+----------+-------+-----------------+-------+------------+-------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|country|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+-------+\n",
      "|     1|   Edith|  100000.0|     35|Software Engineer|      2|    112500.0|  India|\n",
      "|     2|  tyrion|  120000.0|     25|       Programmer|      4|    135000.0|  India|\n",
      "|     3|     jon|   51000.0|     37|  System Engineer|      4|     57375.0|  India|\n",
      "|     4|daenerys|   70000.0|     31|        Developer|      1|     78750.0|  India|\n",
      "|     5|    Andy|   20000.0|     41| Senior Developer|      2|     22500.0|  India|\n",
      "|     6|    Luis|   40000.0|     25|        Developer|      3|     45000.0|  India|\n",
      "|     7|  Keshav|   50000.0|     36|       Programmer|      2|     56250.0|  India|\n",
      "|     8|   david|   60000.0|     42|  System Engineer|      1|     67500.0|  India|\n",
      "|     9|  Madhav|   40000.0|     22|Software Engineer|      3|     45000.0|  India|\n",
      "|    10|   Jadav|   70000.0|     32|Software Engineer|   NULL|     78750.0|  India|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_salary.show()\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# withColumn() to add new column\n",
    "added_column = updated_salary.withColumn(\"country\", lit(\"India\"))\n",
    "added_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aac5ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+-----------------+-------+------------+-------+\n",
      "|emp_id|emp_name|emp_salary|emp_age|  emp_designation|dept_id|hiked_salary|country|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+-------+\n",
      "|     1|   Edith|  100000.0|     35|Software Engineer|      2|    112500.0|  India|\n",
      "|     2|  tyrion|  120000.0|     25|       Programmer|      4|    135000.0|  India|\n",
      "|     3|     jon|   51000.0|     37|  System Engineer|      4|     57375.0|  India|\n",
      "|     4|daenerys|   70000.0|     31|        Developer|      1|     78750.0|  India|\n",
      "|     5|    Andy|   20000.0|     41| Senior Developer|      2|     22500.0|  India|\n",
      "|     6|    Luis|   40000.0|     25|        Developer|      3|     45000.0|  India|\n",
      "|     7|  Keshav|   50000.0|     36|       Programmer|      2|     56250.0|  India|\n",
      "|     8|   david|   60000.0|     42|  System Engineer|      1|     67500.0|  India|\n",
      "|     9|  Madhav|   40000.0|     22|Software Engineer|      3|     45000.0|  India|\n",
      "|    10|   Jadav|   70000.0|     32|Software Engineer|   NULL|     78750.0|  India|\n",
      "+------+--------+----------+-------+-----------------+-------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition()\n",
    "repartitioned_data = added_column.rdd.repartition(6)\n",
    "repartitioned_data.toDF().show()\n",
    "\n",
    "# getNumPartitions()\n",
    "repartitioned_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "292b0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------------+\n",
      "|emp_name|emp_salary|emp_age|hiked_salary|\n",
      "+--------+----------+-------+------------+\n",
      "|  tyrion|  120000.0|     25|    135000.0|\n",
      "|daenerys|   70000.0|     31|     78750.0|\n",
      "+--------+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark SQL queries\n",
    "repartitioned_data.toDF().createOrReplaceTempView(\"employee\")\n",
    "spark_session.sql(\"select emp_name, emp_salary, emp_age, hiked_salary from employee where emp_age < 32 AND hiked_salary > 50000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "522fe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", 36636, \"M\", 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", 40288, \"M\", 4000),\n",
    "    (\"Robert\", \"\",\"Williams\", 42114, \"M\", 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", 39192, \"F\", 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", 49192, \"F\", -1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73ebf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|49192|     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = spark_session.createDataFrame(data, schema=schema)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516934a",
   "metadata": {},
   "source": [
    "# Working with User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed4e44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+------+------+\n",
      "|first_name|last_name|age|gender|young?|\n",
      "+----------+---------+---+------+------+\n",
      "|  srikanth|      m s| 27|  male|  true|\n",
      "|       abc|      xyz| 37|female| false|\n",
      "+----------+---------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "data = [\n",
    "    {\"first_name\": \"srikanth\", \"last_name\": \"m s\", \"age\": 27, \"gender\": \"male\"},\n",
    "    {\"first_name\": \"abc\", \"last_name\": \"xyz\", \"age\": 37, \"gender\": \"female\"},\n",
    "]\n",
    "\n",
    "test_df = spark_session.createDataFrame(data)\n",
    "\n",
    "def age_differentiator(age):\n",
    "    if age < 30: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "age_differentiator_udf =  udf(age_differentiator)\n",
    "test_df.select(col(\"first_name\"), col(\"last_name\"), col(\"age\"),\n",
    "               col(\"gender\"), age_differentiator_udf(col(\"age\")).alias(\"young?\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3893425",
   "metadata": {},
   "source": [
    "# RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7898d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'is', 'srikanth', 'and', 'i', 'am', 'playing', 'with', 'spark', 'and', 'also', 'learning', 'spark', 'RDD']\n"
     ]
    }
   ],
   "source": [
    "# RDDs are immutable which means once you make changes to RDDS you should assign it to new RDDS since it can update existing RDDS\n",
    "\n",
    "text_data = \"Hello this is srikanth and i am playing with spark and also learning spark RDD\"\n",
    "\n",
    "raw_data = text_data.split(\" \")\n",
    "\n",
    "rdd_data = spark_session.sparkContext.parallelize(raw_data)\n",
    "\n",
    "rdd_list = rdd_data.collect()\n",
    "\n",
    "print(rdd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e307a52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'is', 'am', 'spark', 'this', 'Hello', 'srikanth', 'playing', 'with', 'learning', 'i', 'also', 'RDD']\n"
     ]
    }
   ],
   "source": [
    "# distinct() transformations to get distinct RDDS and collect() to collect resultant data as list\n",
    "\n",
    "distinct_rdd = rdd_data.distinct().collect()\n",
    "\n",
    "print(distinct_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f92091ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['srikanth', 'spark', 'spark']\n"
     ]
    }
   ],
   "source": [
    "# filter() transformations for spark RDD\n",
    "\n",
    "def wordStartsWith(word, letter):\n",
    "    return word.startswith(letter)\n",
    "\n",
    "filtered_rdd = rdd_data.filter(lambda word: wordStartsWith(word, 's'))\n",
    "filtered_rdd_list = filtered_rdd.collect()\n",
    "print(filtered_rdd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2881b6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1.0]\n",
      "[2, 4, 4, 1.0]\n",
      "[3, 6, 9, 1.0]\n",
      "[4, 8, 16, 1.0]\n",
      "[5, 10, 25, 1.0]\n",
      "[6, 12, 36, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# map() transformations on RDD\n",
    "\n",
    "num_data = [1, 2, 3, 4, 5, 6]\n",
    "num_rdd = spark_session.sparkContext.parallelize(num_data)\n",
    "\n",
    "mapped_rdd = num_rdd.map(lambda num: [num, num+num, num*num, num/num]).collect()\n",
    "\n",
    "for data in mapped_rdd:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57915e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o', 't', 'h', 'i', 's', 'i', 's', 's', 'r', 'i', 'k']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap() transformation on RDD\n",
    "\n",
    "rdd_data.flatMap(lambda word: list(word)).take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c08435a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Australia', 25), ('India', 91), ('USA', 12)]\n",
      "**************************************************\n",
      "[(12, 'USA'), (25, 'Australia'), (91, 'India')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sortByKey() RDD transformation\n",
    "\n",
    "countries = [('India', 91), ('USA', 12), ('Australia', 25)]\n",
    "countries_rdd = spark_session.sparkContext.parallelize(countries)\n",
    "\n",
    "sorted_countries_list = countries_rdd.sortByKey(ascending=True).collect()\n",
    "print(sorted_countries_list)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# using both map() and sortByKey() together\n",
    "sorted_countries_list = countries_rdd.map(lambda countries_rdd: (countries_rdd[1], countries_rdd[0])) \\\n",
    "                                     .sortByKey(ascending=True).collect()\n",
    "print(sorted_countries_list)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccbde1",
   "metadata": {},
   "source": [
    "# RDD Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b77f311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "6 6\n",
      "12 8\n",
      "20 10\n",
      "**************************************************\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# reduce() action on RDD\n",
    "\n",
    "def getSum(a, b):\n",
    "    print(a, b)\n",
    "    return a+b\n",
    "\n",
    "num = [2,4,6,8,10]\n",
    "num_rdd = spark_session.sparkContext.parallelize(num)\n",
    "\n",
    "res = num_rdd.reduce(lambda x, y: getSum(x, y))\n",
    "print(\"*\"*50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
